{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_list = []\n",
    "\n",
    "rge = range(0,10)\n",
    "d = {i:i for i in rge}\n",
    "df = pd.DataFrame(columns=['url'])\n",
    "for j in range(1, 184):\n",
    "    html = requests.get('https://premium.usnews.com/best-colleges/search?_sort=rank&_sortDirection=asc&_page={}'.format(j), headers = {\n",
    "        \"Host\": \"www.usnews.com\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\"\n",
    "    }).text\n",
    "    soup = BeautifulSoup(html, 'lxml')    \n",
    "    \n",
    "    for i in d:\n",
    "        li = soup.find('li', id = 'school-%s'%i)\n",
    "        \n",
    "        if not li:\n",
    "            break\n",
    "        \n",
    "        h3 = li.h3\n",
    "        url = h3.a['href']\n",
    "        \n",
    "        school_list.append(url)\n",
    "        \n",
    "        \n",
    "data = {\n",
    "    \"school\": school_list\n",
    "}\n",
    "\n",
    "url_list = pd.DataFrame(data)\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "url_list\n",
    "\n",
    "url_list.to_csv('material/urls.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Urls = pd.read_csv('material/urls.csv', names=['University', 'URL'])\n",
    "\n",
    "regular_info = pd.DataFrame(index=Urls['University'])\n",
    "pre_col = ['description', 'Mission & Unique Qualities',\n",
    "           'Types of campus housing available', 'Collegiate athletic association', 'Campus Safety']\n",
    "for i in pre_col:\n",
    "    regular_info[i] = 'N/A'\n",
    "\n",
    "\n",
    "for i in range( len(Urls)):\n",
    "    url = ('https://www.usnews.com' + Urls.iloc[i, 1] + '/student-life').replace(' ', '')\n",
    "    print(url)\n",
    "\n",
    "    session = HTMLSession()\n",
    "    page = session.get(url, timeout=5)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    # description\n",
    "    description = list(soup.select('#summary-box > react-trigger > div > p'))[0].text\n",
    "    regular_info['description'][i] = description\n",
    "\n",
    "\n",
    "    # life stats\n",
    "    life_stats = soup.find(class_='quick-stat-box')\n",
    "    for j in life_stats.find_all('div'):\n",
    "        category = j.find('dt').text\n",
    "        if category not in regular_info.columns:\n",
    "            regular_info[category] = 'N/A'\n",
    "        regular_info[category][i] = j.find('dd').text\n",
    "\n",
    "\n",
    "    # body\n",
    "    body = list(soup.select('#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.izoLfu.mb0 > div > div > div:nth-child(2) > div'))[0]\n",
    "\n",
    "    # School Mission & Unique Qualities\n",
    "    MUQ = list(body.find_all('div', recursive=False))[0].find('div')\n",
    "    regular_info['Mission & Unique Qualities'][i] = MUQ.text.replace('Content is provided by the school.', '')\n",
    "\n",
    "    # Student Body\n",
    "    student_body = list(body.find_all('div', recursive=False))[1].find('div').find_all('div', recursive=False)\n",
    "    for j in student_body:\n",
    "        if len(j.find_all('p')) == 2:\n",
    "            if j.find_all('p')[0].text.strip() not in regular_info.columns:\n",
    "                regular_info[j.find_all('p')[0].text.strip()] = 'N/A'\n",
    "            regular_info[j.find_all('p')[0].text.strip()][i] = j.find_all('p')[1].text\n",
    "        else:\n",
    "            gender = j.find('div').find_all('div', recursive=False)[1].find_all('div')\n",
    "            for k in gender:\n",
    "                res = k.find('b').text\n",
    "                title = k.text.replace(res, '')\n",
    "                if title not in regular_info.columns:\n",
    "                    regular_info[title] = 'N/A'\n",
    "                regular_info[title][i] = res\n",
    "\n",
    "    # Housing & Dorms at Harvard University\n",
    "    housing_dorms = list(body.find_all('div', recursive=False))[2].find('div')\n",
    "    if len(list(housing_dorms)[0].find_all('p')) == 2:\n",
    "        j = list(housing_dorms)[0]\n",
    "        if j.find_all('p')[0].text.strip() not in regular_info.columns:\n",
    "            regular_info[j.find_all('p')[0].text.strip()] = 'N/A'\n",
    "        regular_info[j.find_all('p')[0].text.strip()][i] = j.find_all('p')[1].text\n",
    "    else:\n",
    "        live = list(housing_dorms)[0].find('div').find_all('div', recursive=False)[1].find_all('div')\n",
    "        for k in live:\n",
    "            res = k.find('b').text\n",
    "            title = k.text.replace(res, '')\n",
    "            if title not in regular_info.columns:\n",
    "                regular_info[title] = 'N/A'\n",
    "            regular_info[title][i] = res\n",
    "\n",
    "    if len(list(housing_dorms)[1].find_all('p')) == 2:\n",
    "        regular_info['Types of campus housing available'][i] = list(housing_dorms)[1].find_all('p')[0].text\n",
    "    else:\n",
    "        types = list(housing_dorms)[1].find('div').find_all('li')\n",
    "        res = []\n",
    "        for k in types:\n",
    "            res.append(k.text)\n",
    "        res = 'ï¼Œ '.join(res)\n",
    "        regular_info['Types of campus housing available'][i] = res\n",
    "\n",
    "    # sports\n",
    "    sports = list(body.find_all('div', recursive=False))[3]\n",
    "    regular_info['Collegiate athletic association'][i] = sports.div.div.find_all('p')[1].text\n",
    "\n",
    "    # Clubs & Organizations\n",
    "    clubs_organizations = list(body.find_all('div', recursive=False))[4].find('div', recursive=False)\n",
    "    for j in clubs_organizations:\n",
    "        if len(j.find_all('p')) == 2:\n",
    "            title = j.find_all('p')[0].text\n",
    "            res = j.find_all('p')[1].text\n",
    "            if title not in regular_info.columns:\n",
    "                regular_info[title] = 'N/A'\n",
    "            regular_info[title][i] = res\n",
    "        else:\n",
    "            gender = j.find('p').text\n",
    "            classify = j.find('div').find_all('div', recursive=False)[1].find_all('div')\n",
    "            for k in classify:\n",
    "                res = k.find('b').text\n",
    "                title = k.text.replace(res, '')\n",
    "                if title not in regular_info.columns:\n",
    "                    regular_info[gender + title] = 'N/A'\n",
    "                regular_info[gender + title][i] = res\n",
    "\n",
    "\n",
    "    # safety\n",
    "    safety = list(soup.select('#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.izoLfu.mb0 > div > div > react-trigger > div > div.section-box'))[0]\n",
    "    regular_info['Campus Safety'][i] = safety.find('p').text\n",
    "\n",
    "\n",
    "    # # criminal\n",
    "    # criminal = list(soup.select('#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.izoLfu.mb0 > div > div > react-trigger > div > react-trigger'))[0]\n",
    "    # for j in criminal:\n",
    "    # \tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are full of 'N/A'\n",
    "for i in range(len(regular_info.columns)):\n",
    "    if len(set(regular_info.iloc[:, i])) == 1:\n",
    "        print(i)\n",
    "        print(regular_info.columns[i])\n",
    "\n",
    "regular_info = regular_info.drop(['Students living on- and off-campus', 'Degree-seeking student gender distribution',\n",
    "                                  'Undergraduate men who are members of a fraternity',\n",
    "                                  'Undergraduate women who are members of a sorority'], axis=1)\n",
    "\n",
    "\n",
    "# roughly clean the data\n",
    "def clean(column):\n",
    "    for m in range(len(regular_info)):\n",
    "        temp = regular_info[column][m].split('\\n')\n",
    "        for n in range(len(temp)):\n",
    "            temp[n] = temp[n].strip()\n",
    "        temp = ' '.join(temp)\n",
    "        regular_info[column][m] = temp\n",
    "\n",
    "\n",
    "clean('description')\n",
    "clean('Campus Safety')\n",
    "\n",
    "\n",
    "# regular_info.to_csv('../../user-notebooks/yjin/USNews/results/student life.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get categories and other prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Urls = pd.read_csv('material/urls.csv', names=['University', 'URL'])\n",
    "headers = {\n",
    "    \"Host\": \"www.usnews.com\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url = \"https://www.usnews.com/best-colleges/search\"\n",
    "proxy = \"127.0.0.1:8866\"\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.text\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "sel = soup.find_all(\"select\", id='schoolType')\n",
    "school_type_options = list(sel)[0].find_all(\"option\")\n",
    "school_type_options_text = [i.text for i in school_type_options][1:]\n",
    "school_type_options_value = [i.get('value') for i in school_type_options][1:]\n",
    "\n",
    "\n",
    "def get_ranks(lin):\n",
    "    out = []\n",
    "    for i in range(1, 100):\n",
    "        resp = requests.get(lin + '?_page=' + str(i), headers=headers)\n",
    "        if resp.status_code == 404:\n",
    "            break\n",
    "\n",
    "        s = BeautifulSoup(resp.text, 'lxml')\n",
    "        for j in range(0, 10):\n",
    "            l_i = s.find('li', id='school-' + str(j))\n",
    "            if not l_i:\n",
    "                break\n",
    "            out.append(l_i.h3.a.text)\n",
    "    return out\n",
    "\n",
    "ranking_df = pd.DataFrame(index=Urls['University'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape and store results to csv table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in school_type_options_value:\n",
    "    link = 'https://www.usnews.com/best-colleges/rankings/{0}'.format(st)\n",
    "    if st == 'business':\n",
    "        link = link + '-overall'\n",
    "    school_name = get_ranks(link)\n",
    "    ranking_df[st.replace('-', ' ')] = 0\n",
    "    for i in range(len(school_name)):\n",
    "        ranking_df.loc[school_name[i], st.replace('-', ' ')] = i\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(link, headers=headers).text, 'html.parser')\n",
    "    ranking_options = soup.find_all(\"select\", id='ranking')[0].find_all(\"option\")\n",
    "    ranking_options_text = [i.text for i in ranking_options][1:]\n",
    "    ranking_options_value = [i.get('value') for i in ranking_options][1:]\n",
    "\n",
    "\n",
    "    for r in ranking_options_value:\n",
    "        if '-' in st:\n",
    "            link = 'https://www.usnews.com/best-colleges/rankings/{0}/{1}'.format(st, r)\n",
    "        else:\n",
    "            link = 'https://www.usnews.com/best-colleges/rankings/{0}-{1}'.format(st, r)\n",
    "        school_name = get_ranks(link)\n",
    "\n",
    "        col_name = st.replace('-', ' ') + ' - ' + r.replace('-', ' ')\n",
    "        ranking_df[col_name] = 0\n",
    "        for i in range(len(school_name)):\n",
    "            ranking_df.loc[school_name[i], col_name] = i + 1\n",
    "\n",
    "# ranking_df.to_csv('results/ranking.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get USNews ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Urls = pd.read_csv('material/urls.csv', header=None)\n",
    "Urls.columns = ['University', 'Url']\n",
    "Urls['USNews_ID'] = [i[-4:] for i in urls.iloc[:, 1]]\n",
    "# Urls.to_csv('../../user-notebooks/yjin/USNews/results/USNewsID.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuition and Financial Aid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape general tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = pd.DataFrame(columns=[\"University\", \"Content\"])\n",
    "texts = []\n",
    "\n",
    "for i in range(len(Urls)):\n",
    "    url = ('https://www.usnews.com' + Urls.iloc[i, 1] + '/paying').replace(' ', '')\n",
    "    session = HTMLSession()\n",
    "    content = session.get(url)\n",
    "    basic_address = \"#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > \" \\\n",
    "                    \"div > div > div:nth-child(2) > div\"\n",
    "\n",
    "    # Tuition & Expenses\n",
    "    temp_sector = basic_address + \" > div:nth-child(1) > div:nth-child(2)\"\n",
    "    results = content.html.find(temp_sector)\n",
    "    text = results[0].text\n",
    "    texts.append(text)\n",
    "\n",
    "content_df['University'] = Urls['University']\n",
    "content_df['Content'] = texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## format the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = content_df\n",
    "average_annual_cost_by_family_income = pd.DataFrame(columns=['$0-$30,000', '$30,001-$48,000', '$48,001-$75,000',\n",
    "                                                             '$75,001-$110,000', '$110,001+'], index=data['University'])\n",
    "general_cost = pd.DataFrame(columns=['Tuition and fees', 'In-state tuition and fees', 'Out-of-state tuition and fees',\n",
    "                                     'Average annual cost', 'Room and board'], index=data['University'])\n",
    "\n",
    "for i in range(len(data)):\n",
    "    check = re.findall('\\n\\(\\d\\d\\d\\d-\\d\\d\\)', data.iloc[i, 1])\n",
    "    for element in set(check):\n",
    "        data.iloc[i, 1] = data.iloc[i, 1].replace(element, '')\n",
    "    temp = np.array(data.iloc[i, 1].split('\\n'))\n",
    "    help_ind = np.where(temp == 'Average annual cost by family income')[0][0]\n",
    "    j = 0\n",
    "    temp = np.array(data.iloc[i, 1].split('\\n'))\n",
    "    while True:\n",
    "        if j == help_ind:\n",
    "            average_annual_cost_by_family_income.iloc[i, :] = temp[[k + j for k in [14, 16, 18, 20, 22]]]\n",
    "            j += 23\n",
    "        elif temp[j] != \"Go to this school's net price calculator\":\n",
    "            general_cost[temp[j]][i] = temp[j + 1]\n",
    "            j += 2\n",
    "        else:\n",
    "            break\n",
    "\n",
    "general_cost = general_cost.fillna('N/A')\n",
    "average_annual_cost_by_family_income = average_annual_cost_by_family_income.fillna('N/A')\n",
    "\n",
    "# average_annual_cost_by_family_income.to_csv('../../user-notebooks/yjin/USNews/results/average_annual_cost_by_family_income.csv')\n",
    "# general_cost.to_csv('../../user-notebooks/yjin/USNews/results/general_cost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape quick status in Tuition and Financial Aid block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_stats_df = pd.DataFrame(index=Urls['University'])\n",
    "\n",
    "for i in range(len(Urls)):\n",
    "    url = ('https://www.usnews.com' + Urls.iloc[i, 1] + '/paying').replace(' ', '')\n",
    "    session = HTMLSession()\n",
    "    content = session.get(url)\n",
    "    basic_address = \"#app > div > div:nth-child(1) > div.Heading__ProfileHeadingBox-sc-6a32fq-5.fafMtc > div > div > \" \\\n",
    "                    \"div > div.Villain__FlexDiv-sc-1y12ps5-0.ckPKDS > div.Villain__SupplementColumn-sc-1y12ps5-2.kKLGsQ > \" \\\n",
    "                    \"div > div > div\"\n",
    "    results_0 = content.html.find(basic_address)\n",
    "    if len(results_0) > 0:\n",
    "        text = results_0[0].text\n",
    "        n = text.count('\\n', 0, len(text))\n",
    "        for j in range(1, n + 1):\n",
    "            item_path = basic_address + '> div.Panel__Content-ejd83f-0.ftmBcb.border-right.border-left.border-bottom > ' \\\n",
    "                                        'table > tbody > tr:nth-child({0}) > div > a.Anchor-byh49a-0.iPzQVi'.format(j)\n",
    "            temp_item = content.html.find(item_path)[0].text\n",
    "            value_path = item_path.replace('a.Anchor-byh49a-0.iPzQVi', 'a.Anchor-byh49a-0.gbsGda')\n",
    "            temp_value = content.html.find(value_path)[0].text\n",
    "            if temp_item in quick_stats_df.columns:\n",
    "                quick_stats_df[temp_item][i] = temp_value\n",
    "            else:\n",
    "                quick_stats_df[temp_item] = 'N/A'\n",
    "                quick_stats_df[temp_item][i] = temp_value\n",
    "\n",
    "# quick_stats_df.to_csv('../../user-notebooks/yjin/USNews/results\\FA quick stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape the student loan block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape the text part (student loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_loan = pd.DataFrame(columns=[\"University\"], index=Urls['University'])\n",
    "texts = []\n",
    "\n",
    "\n",
    "for i in range(len(Urls)):\n",
    "    url = ('https://www.usnews.com' + Urls.iloc[i, 1] + '/paying').replace(' ', '')\n",
    "    # print(url)\n",
    "    session = HTMLSession()\n",
    "    content = session.get(url)\n",
    "    basic_address = \"#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > div > \" \\\n",
    "                    \"div > div:nth-child(2) > div > div:nth-child(3) > div:nth-child(3)\"\n",
    "    results_0 = content.html.find(basic_address)\n",
    "    if len(results_0) > 0:\n",
    "        text = results_0[0].text\n",
    "        texts.append(text)\n",
    "    else:\n",
    "        texts.append('N/A')\n",
    "\n",
    "student_loan['Content'] = texts\n",
    "# student_loan.to_csv('../../user-notebooks/yjin/USNews/data\\student loan text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape the tables (student loanï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Typical_total_federal_loan_debt_by_family_income = pd.DataFrame(index=Urls['University'])\n",
    "Undergraduates_federal_loan_debt_by_family_income = pd.DataFrame(index=Urls['University'])\n",
    "general_df = pd.DataFrame(index=Urls['University'])\n",
    "\n",
    "for i in range(len(Urls)):\n",
    "    url = ('https://www.usnews.com' + Urls.iloc[i, 1] + '/paying').replace(' ', '')\n",
    "    print(url)\n",
    "    session = HTMLSession()\n",
    "    content = session.get(url)\n",
    "    basic_address = \"#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > div > \" \\\n",
    "                    \"div > div:nth-child(2) > div > div:nth-child(3) > div:nth-child(3)\"\n",
    "    j = 1\n",
    "    # not sure how much sub blocks are there so use while loop\n",
    "    while True:\n",
    "        temp_path = basic_address + '> div:nth-child({0})'.format(j)\n",
    "        results_0 = content.html.find(temp_path)\n",
    "        # if the block exists\n",
    "        if len(results_0) > 0:\n",
    "            text = results_0[0].text\n",
    "            temp = text.split('\\n')\n",
    "\n",
    "            # if not the sub tables\n",
    "            if len(temp) == 2:\n",
    "                # if column exists, and if not\n",
    "                if temp[0] in general_df.columns:\n",
    "                    general_df[temp[0]][i] = temp[1]\n",
    "                else:\n",
    "                    general_df[temp[0]] = 'N/A'\n",
    "                    general_df[temp[0]][i] = temp[1]\n",
    "\n",
    "            # see which sub table it belongs to\n",
    "            else:\n",
    "                n = int((len(temp)-3)/4)\n",
    "                if temp[0] == 'Typical total federal loan debt by family income':\n",
    "                    for k in range(n):\n",
    "                        if temp[2*(k+n) + 3] in Typical_total_federal_loan_debt_by_family_income.columns:\n",
    "                            Typical_total_federal_loan_debt_by_family_income[temp[2*(k+n) + 3]][i] = temp[2*(k+n) + 4]\n",
    "                        else:\n",
    "                            Typical_total_federal_loan_debt_by_family_income[temp[2*(k+n) + 3]] = 'N/A'\n",
    "                            Typical_total_federal_loan_debt_by_family_income[temp[2*(k+n) + 3]][i] = temp[2*(k+n) + 4]\n",
    "                elif temp[0] == 'Undergraduates paying down their federal loan debt by family income':\n",
    "                    for k in range(n):\n",
    "                        if temp[2*(k+n) + 3] in Undergraduates_federal_loan_debt_by_family_income.columns:\n",
    "                            Undergraduates_federal_loan_debt_by_family_income[temp[2*(k+n) + 3]][i] = temp[2*(k+n) + 4]\n",
    "                        else:\n",
    "                            Undergraduates_federal_loan_debt_by_family_income[temp[2*(k+n) + 3]] = 'N/A'\n",
    "                            Undergraduates_federal_loan_debt_by_family_income[temp[2*(k+n) + 3]][i] = temp[2*(k+n) + 4]\n",
    "        else:\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "# Typical_total_federal_loan_debt_by_family_income.to_csv('../../user-notebooks/yjin/USNews/results/Student Loan Debt by family income(Typical total).csv')\n",
    "# Undergraduates_federal_loan_debt_by_family_income.to_csv('../../user-notebooks/yjin/USNews/results/Student Loan Debt by family income(Undergraduates).csv')\n",
    "# general_df.to_csv('../../user-notebooks/yjin/USNews/results/Student Loan Debt(general).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in and scrape campus regular info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "                         ' Chrome/84.0.4147.105 Safari/537.36',\n",
    "           'username': 'your username',\n",
    "           'password': 'your password',\n",
    "           # you may need to change cookie here:\n",
    "           # log in on your browser, find it in the 'Network' section, 'Request Headers' block, with developer tool\n",
    "           \"cookie\": 'get with developer tool'}\n",
    "regular_info = pd.DataFrame(index=Urls['University'])\n",
    "\n",
    "for i in range(len(Urls)):\n",
    "    url = ('https://www.usnews.com' + Urls.iloc[i, 1] + '/campus-info').replace(' ', '')\n",
    "    print(url)\n",
    "\n",
    "    session = HTMLSession()\n",
    "    page = session.get(url, headers=headers, timeout=5)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    body = soup.find_all('div', class_='Display__BellowStyled-h3gn08-6 gdKeTK')\n",
    "\n",
    "    # get main sections title:\n",
    "    ID = []\n",
    "    for section in body:\n",
    "        ID.append(section['id'])\n",
    "\n",
    "    # map main sections and get info inside\n",
    "    for j in range(len(ID)):\n",
    "        k = 1\n",
    "        while True:\n",
    "            test = soup.select('#{0} > div > div > div:nth-child({1})'.format(ID[j], k))\n",
    "            if len(test) == 0:  # which means there's no next block in this section\n",
    "                break  # therefore come to the next section\n",
    "            test_children = list(test[0].children)\n",
    "            aa = test[0].find_all('div', recursive=False)\n",
    "\n",
    "            for x in aa:\n",
    "                temp = []\n",
    "                for y in x.strings:\n",
    "                    temp.append(y)\n",
    "                if 'Jump To Section:' not in temp:\n",
    "                    col_name = ' - '.join([ID[j], temp[0]])\n",
    "                    if col_name not in regular_info.columns:  # create a new column if the category doesn't exist\n",
    "                        regular_info[col_name] = 'N/A'\n",
    "                    if len(temp) == 2:\n",
    "                        regular_info[col_name][i] = temp[1]\n",
    "                    else:\n",
    "                        regular_info[col_name][i] = ', '.join(temp[1:])\n",
    "                    k += 1\n",
    "                else:  # if it's a button div which doesn't include any useful info, we skip\n",
    "                    k += 1\n",
    "                    break\n",
    "\n",
    "# regular_info.to_csv('../../user-notebooks/yjin/USNews/results/general info in campus block.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links of Images shown in Campus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to help mimicing user actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_handler(brs):\n",
    "    try:\n",
    "        s_window = brs.find_element_by_xpath(\"/html/body/div[3]/div/div/button\")\n",
    "        s_window.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        s_window = brs.find_element_by_xpath(\"/html/body/div[5]/div/div/button\")\n",
    "        s_window.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        s_window = brs.find_element_by_xpath('/html/body/div[6]/div/div/button')\n",
    "        s_window.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def window_handler2(brs):\n",
    "    try:\n",
    "        s_window = brs.find_element_by_xpath(\"/html/body/div[3]/div/div/button\")\n",
    "        s_window.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        s_window = brs.find_element_by_xpath(\"/html/body/div[5]/div/div/button\")\n",
    "        s_window.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        s_window = brs.find_element_by_css_selector(\n",
    "            '#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > div > div.Generic__ContentBox-sc-1itie0w-0.cPiuYB.content > div.section-box > div.mt6 > react-trigger > div > div.Slideshow__Container-bw1gs6-0.gCuKqa > div.Arrows__Container-sc-1s7bkvt-2.jAYcej > button:nth-child(3)')\n",
    "        s_window.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        s_window = brs.find_element_by_xpath('/html/body/div[6]/div/div/button')\n",
    "        s_window.click()\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main part of the scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Urls = pd.read_csv('material/urls.csv', names=['University', 'URL'])\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "\n",
    "img_df = pd.DataFrame(index=Urls['University'])\n",
    "for i in range(24):\n",
    "    col_name = 'Img_{0}'.format(i)\n",
    "    img_df[col_name] = 'N/A'\n",
    "record_df = pd.read_csv('campus/image count.csv', index_col=0)\n",
    "img_df = pd.read_csv('campus/image links new 8.csv', index_col=0).fillna('N/A')\n",
    "\n",
    "\n",
    "for i in range(1, len(Urls)):\n",
    "    url = ('https://www.usnews.com' + Urls.iloc[i, 1] + '/campus-info').replace(' ', '')\n",
    "    browser = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n",
    "    browser.get(url)\n",
    "\n",
    "    content = session.get(url)\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "    basic_address = \"#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > div > \" \\\n",
    "                    \"div > div.section-box > div.mt6\"\n",
    "    results_0 = content.html.find(basic_address)\n",
    "\n",
    "    if len(results_0) > 0:\n",
    "        text = results_0[0].text\n",
    "        a = re.search('\\d.*of.*\\d', text)\n",
    "        N = int(a.group().split(' of ')[1].split('\\n')[0])\n",
    "        record_df.iloc[i, 0] = N\n",
    "\n",
    "        # if numbers of pictures are more then previous setting\n",
    "        if N > img_df.shape[1]:\n",
    "            for n in range(img_df.shape[1], N):\n",
    "                col_name = 'Img_{0}'.format(n)\n",
    "                img_df[col_name] = 'N/A'\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                element1 = soup.find_all(\"img\", {\"class\": \"SlideshowInContent__SlideImage-ecnkfw-2 tSQUx\"})[0]\n",
    "            except:\n",
    "                try:\n",
    "                    window_handler(browser)\n",
    "                    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "                except:\n",
    "                    window_handler(browser)\n",
    "                    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        try:\n",
    "            link = element1['src']\n",
    "        except:\n",
    "            link = 'EMPTY'\n",
    "        img_df.iloc[i, 0] = link\n",
    "\n",
    "        # map all images\n",
    "        n = 1\n",
    "        if N >= 2:\n",
    "            while img_df.iloc[i, N - 1] == 'N/A':\n",
    "                try:\n",
    "                    page_next = browser.find_element_by_css_selector(\n",
    "                        '#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > '\n",
    "                        'div > div > div.section-box > div.mt6 > react-trigger > div > '\n",
    "                        'div.Slideshow__Container-bw1gs6-0.gCuKqa > div.Arrows__Container-sc-1s7bkvt-2.jAYcej > '\n",
    "                        'button:nth-child(3)')\n",
    "                    page_next.click()\n",
    "                except:\n",
    "                    # time.sleep(2)\n",
    "                    window_handler(browser)\n",
    "                    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "                    count = browser.find_element_by_css_selector(\n",
    "                        '#app > div > div:nth-child(1) > div:nth-child(8) > div > '\n",
    "                        'div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > div > '\n",
    "                        'div.Generic__ContentBox-sc-1itie0w-0.cPiuYB.content > '\n",
    "                        'div.section-box > div.mt6 > react-trigger > div > '\n",
    "                        'div.Slideshow__Container-bw1gs6-0.gCuKqa > '\n",
    "                        'div.Slideshow__SlideContainer-bw1gs6-1.kcTrLh > div > span')\n",
    "                    compare = count.text.split(' of ')\n",
    "                    print(compare)\n",
    "                    if compare[0] != compare[1] and int(compare[0]) <= n:\n",
    "                        print('mark 8-1')\n",
    "                        page_next = browser.find_element_by_css_selector(\n",
    "                            '#app > div > div:nth-child(1) > div:nth-child(8) > div > div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > '\n",
    "                            'div > div > div.section-box > div.mt6 > react-trigger > div > '\n",
    "                            'div.Slideshow__Container-bw1gs6-0.gCuKqa > div.Arrows__Container-sc-1s7bkvt-2.jAYcej > '\n",
    "                            'button:nth-child(3)')\n",
    "                        print('mark 8-2')\n",
    "                        location = page_next.location\n",
    "                        size = page_next.size\n",
    "                        print('mark 8-3')\n",
    "                        browser.execute_script(\"window.scrollTo({0}, {1})\".format(0, location['y']-size['height']*2))\n",
    "                        print('mark 8-4')\n",
    "                        while True:\n",
    "                            try:\n",
    "                                page_next.click()\n",
    "                            except:\n",
    "                                window_handler(browser)\n",
    "                            else:\n",
    "                                break\n",
    "                    else:\n",
    "                        print('mark 8-break')\n",
    "                        break\n",
    "\n",
    "                print(\"link and ...\")\n",
    "                while True:\n",
    "                    try:\n",
    "                        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "                        element1 = \\\n",
    "                            soup.find_all(\"img\", {\"class\": \"SlideshowInContent__SlideImage-ecnkfw-2 tSQUx\"})[0]\n",
    "                        count = browser.find_element_by_css_selector(\n",
    "                            '#app > div > div:nth-child(1) > div:nth-child(8) > div > '\n",
    "                            'div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > div > '\n",
    "                            'div.Generic__ContentBox-sc-1itie0w-0.cPiuYB.content > '\n",
    "                            'div.section-box > div.mt6 > react-trigger > div > '\n",
    "                            'div.Slideshow__Container-bw1gs6-0.gCuKqa > '\n",
    "                            'div.Slideshow__SlideContainer-bw1gs6-1.kcTrLh > div > span')\n",
    "                        n = int(count.text.split(' of ')[0]) - 1\n",
    "                    except:\n",
    "                        try:\n",
    "                            window_handler(browser)\n",
    "                            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "                            element1 = \\\n",
    "                                soup.find_all(\"img\", {\"class\": \"SlideshowInContent__SlideImage-ecnkfw-2 tSQUx\"})[0]\n",
    "                            count = browser.find_element_by_css_selector(\n",
    "                                '#app > div > div:nth-child(1) > div:nth-child(8) > div > '\n",
    "                                'div.Cell-sc-1abjmm4-0.kXBDnR.mb0 > div > '\n",
    "                                'div.Generic__ContentBox-sc-1itie0w-0.cPiuYB.content > '\n",
    "                                'div.section-box > div.mt6 > react-trigger > div > '\n",
    "                                'div.Slideshow__Container-bw1gs6-0.gCuKqa > '\n",
    "                                'div.Slideshow__SlideContainer-bw1gs6-1.kcTrLh > div > span')\n",
    "                            n = int(count.text.split(' of ')[0]) - 1\n",
    "                        except:\n",
    "                            window_handler2(browser)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                try:\n",
    "                    link = element1['src']\n",
    "                except:\n",
    "                    link = 'EMPTY'\n",
    "                if img_df.iloc[i, n] == 'N/A':\n",
    "                    img_df.iloc[i, n] = link\n",
    "                print('Time {0} finished'.format(n))\n",
    "                n += 1\n",
    "\n",
    "    browser.close()\n",
    "\n",
    "\n",
    "# img_df.to_csv('../../user-notebooks/yjin/USNews/results/image links.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
